{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11078221,"sourceType":"datasetVersion","datasetId":6903514}],"dockerImageVersionId":31012,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.api as sm\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import make_scorer, r2_score\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-11T10:29:58.078861Z","iopub.execute_input":"2025-04-11T10:29:58.079220Z","iopub.status.idle":"2025-04-11T10:29:58.095643Z","shell.execute_reply.started":"2025-04-11T10:29:58.079197Z","shell.execute_reply":"2025-04-11T10:29:58.094850Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/datos-precios-viviendas-y-mas-espaa/datos_unidos_meses.csv')\ndf_edad = pd.read_csv('/kaggle/input/datos-precios-viviendas-y-mas-espaa/df_mineria_datos_edades.csv')\ndf_vivienda = pd.read_csv('/kaggle/input/datos-precios-viviendas-y-mas-espaa/df_mineria_datos_vivienda.csv')#le doy una variable pero para este estudio no voy a utilizar\n#esta parte del dataset ya que da datos geograficos","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T10:29:58.096942Z","iopub.execute_input":"2025-04-11T10:29:58.097178Z","iopub.status.idle":"2025-04-11T10:29:58.250183Z","shell.execute_reply.started":"2025-04-11T10:29:58.097159Z","shell.execute_reply":"2025-04-11T10:29:58.249390Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print('INFORAMCION DE CADA DATASET A UTILIZAR Y PRIMERAS LINEAS')\n\nprint('datos unidos:')\nprint(df.info())\nprint(df.head())\n\nprint('datos edades:')\nprint(df_edad.info())\nprint(df_edad.head())\n\nprint(df.columns)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T10:29:58.251154Z","iopub.execute_input":"2025-04-11T10:29:58.251517Z","iopub.status.idle":"2025-04-11T10:29:58.279527Z","shell.execute_reply.started":"2025-04-11T10:29:58.251488Z","shell.execute_reply":"2025-04-11T10:29:58.278717Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Editamos el dataset para hacerlo mas comodo para su uso\n\n#revisamos elementos nulos y que no haya duplicados\n\nprint(\"\\nMissing Values in Each Column:\")\nprint(df.isnull().sum())\n\n\nprint(\"\\nDuplicate Rows:\", df.duplicated().sum())\ndf.drop_duplicates(inplace=True)\n\n\n#convertimos el formato de la columan year a uno de date y ponemos 2 columnas con mes y año\ndf['year'] = pd.to_datetime(df['year'])\ndf['anho'] = df['year'].dt.year\ndf['month'] = df['year'].dt.month\n\n#hacemos una nueva columna con el precio medio de todos los materiales para mas comodidad\n\ndf['precio_mats'] = df[['precio_Acero','precio_Aluminio','precio_Cemento','precio_Ceramica','precio_Cobre','precio_Energía','precio_Ligantes','precio_Madera']].mean(axis=1)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T10:29:58.281341Z","iopub.execute_input":"2025-04-11T10:29:58.281603Z","iopub.status.idle":"2025-04-11T10:29:58.311502Z","shell.execute_reply.started":"2025-04-11T10:29:58.281584Z","shell.execute_reply":"2025-04-11T10:29:58.310859Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#comprobamos de nuevo el dataset y tambien revisamos cada columna\nprint('dataset editado')\nprint(df.info())\nprint(df.head())\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T10:29:58.312387Z","iopub.execute_input":"2025-04-11T10:29:58.312726Z","iopub.status.idle":"2025-04-11T10:29:58.331753Z","shell.execute_reply.started":"2025-04-11T10:29:58.312700Z","shell.execute_reply":"2025-04-11T10:29:58.331086Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#comprobaciones estadisticas basicas\n\nprint('Summary statistics:')\nprint(df.describe())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T10:29:58.332805Z","iopub.execute_input":"2025-04-11T10:29:58.333074Z","iopub.status.idle":"2025-04-11T10:29:58.380625Z","shell.execute_reply.started":"2025-04-11T10:29:58.333050Z","shell.execute_reply":"2025-04-11T10:29:58.379727Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Comparacion de varios datos segun comunidad ","metadata":{}},{"cell_type":"code","source":"var1 = ['precio', 'IPC', 'edad_media', 'num_hipotecas', 'num_inmigracion', 'precio_mats']\n\nfig, axes = plt.subplots(3, 2, figsize=(12, 4*3))\n\nfor i, var in enumerate(var1):\n    row, col = divmod(i, 2)\n    sns.boxplot(x='comunidad', y=var, data=df, ax=axes[row, col])\n    axes[row, col].set_xticklabels(axes[row, col].get_xticklabels(), rotation=45, ha='right')\n\nfor i in range(len(var1), len(axes)):\n    fig.delaxes(axes[i])\n\nfig.suptitle('Distribución de  variables segun Comunidad', fontweight='bold', fontsize=15)\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T10:29:58.381617Z","iopub.execute_input":"2025-04-11T10:29:58.381939Z","iopub.status.idle":"2025-04-11T10:30:00.237449Z","shell.execute_reply.started":"2025-04-11T10:29:58.381913Z","shell.execute_reply":"2025-04-11T10:30:00.236424Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Por lo que se puede observar tanto el ipc como los precios de los materiales no cambian mucho de una comunidad a otra mientras que la edad varia bastante,el precio segun se observa\nes bastante constante en la mayoria de comunidades salvo en 4 donde se dispara:pais vasco,islas canarias y baleares,madrid y cataluña.Por ultimo podemos observar que la inmigracion es muy desigual\nestando la gran mayoria en la comunidad valenciana","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\nvar1 = ['precio', 'IPC', 'edad_media', 'num_hipotecas', 'num_inmigracion','precio_mats']\n\nfig, axes = plt.subplots(2, 3, figsize=(16, 10), sharex=True, sharey=False)\n\naxes = axes.flatten()\n\nscatter = sns.scatterplot(x='anho', y=var1[0], hue='comunidad', data=df, s=50, alpha=0.8, ax=axes[0])\n\nhandles, labels = axes[0].get_legend_handles_labels()\naxes[0].legend_.remove() \n\nfor i, var in enumerate(var1):\n    ax = axes[i]\n    sns.scatterplot(x='anho', y=var, hue='comunidad', data=df, s=50, alpha=0.8, ax=ax, legend=False)\n    sns.regplot(x='anho', y=var, data=df, order=3, scatter=False, color='black', line_kws={'linewidth': 2}, ax=ax)\n    \n    ax.set_title(f\"{var}\")\n    ax.set_xlabel(\"Año\")\n    ax.set_ylabel(var)\n\nfor i in range(len(var1), len(axes)):\n    fig.delaxes(axes[i])\n\nfig.legend(handles, labels, title=\"Comunidad\", bbox_to_anchor=(1.02, 0.5), loc='center left')\n\nplt.suptitle(\"Evolución  durante los años por comunidades\", fontsize=14)\nplt.tight_layout(rect=[0, 0, 0.85, 1])  \nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T10:30:00.239011Z","iopub.execute_input":"2025-04-11T10:30:00.239271Z","iopub.status.idle":"2025-04-11T10:30:04.858775Z","shell.execute_reply.started":"2025-04-11T10:30:00.239252Z","shell.execute_reply":"2025-04-11T10:30:04.857897Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"En estas graficas podemos observar como sobretodo en el 2010 hubo un aumento drastico tanto del precio coomo de la inmigracion aunque en los años sigueintes se normalizo,sin embargo el ipc y los precios\nde los materiales no han dejado de subir en ningun momento\nEl numero de hipotecas tambein fue muy acusado entre 2005 y 2010 pero desde ahi ha bajado de forma drastica.\nAdemas la edad media tamebin ha subido en todas las comunidades de forma constante aunque no de forma tan brusca como el ipc","metadata":{}},{"cell_type":"code","source":"#mapa de correlacion\n\n\nplt.figure(figsize=(10, 8))\nnum = df[['codigo_comunidad','precio', 'IPC', 'edad_media', 'num_hipotecas', 'num_inmigracion','precio_mats']].corr()\nsns.heatmap(num, annot=True, cmap='coolwarm',fmt=\".2f\")\nplt.title('Correlation Heatmap of Numeric Features')\nplt.tight_layout()\nplt.show()\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T10:30:04.859616Z","iopub.execute_input":"2025-04-11T10:30:04.859849Z","iopub.status.idle":"2025-04-11T10:30:05.243479Z","shell.execute_reply.started":"2025-04-11T10:30:04.859832Z","shell.execute_reply":"2025-04-11T10:30:05.241537Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# MODELO DE REGRESION LINEAL","metadata":{}},{"cell_type":"code","source":"#modelo predictivo\n\n# Predictive Modeling for Income based on Key Variables\n# Selecting relevant features\nfeatures = ['codigo_comunidad','IPC', 'edad_media', 'num_hipotecas', 'num_inmigracion','precio_mats']\ntarget = 'precio'\n\ndf_filtered = df[features + [target]].dropna()  # Remove missing values\n\nX = df_filtered[features]\ny = df_filtered[target]\n\n# Adding a constant for the intercept\nX = sm.add_constant(X)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T10:30:05.247671Z","iopub.execute_input":"2025-04-11T10:30:05.247969Z","iopub.status.idle":"2025-04-11T10:30:05.259125Z","shell.execute_reply.started":"2025-04-11T10:30:05.247948Z","shell.execute_reply":"2025-04-11T10:30:05.258238Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#modelo de regresion lineal\n\nmodel = sm.OLS(y, X).fit()\nprint(model.summary())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T10:30:05.260033Z","iopub.execute_input":"2025-04-11T10:30:05.260316Z","iopub.status.idle":"2025-04-11T10:30:05.286268Z","shell.execute_reply.started":"2025-04-11T10:30:05.260285Z","shell.execute_reply":"2025-04-11T10:30:05.285217Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#modelo de regresion lineal con entreno de variables\n\n# Predictive Modeling: Predicting Housing Prices\n\n# For our simple linear regression, we select a set of predictive features.\n# We remove non-numeric and identifier features.\nfeatures = ['codigo_comunidad','IPC','edad_media', 'mano_obra','num_hipotecas', 'num_inmigracion','precio_mats']\n\n# Only select rows that have no missing values in the features and target columns\ndf_model = df.dropna(subset=features + ['precio'])\n\nX = df_model[features]\ny = df_model['precio']\n\n# Split into training and testing sets (80/20 split)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Initialize and train the model\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\n\n# Make predictions\ny_pred = model.predict(X_test)\n\n# Evaluate the model\nr2 = r2_score(y_test, y_pred)\nmae = mean_absolute_error(y_test, y_pred)\nmse = mean_squared_error(y_test, y_pred)\n\nprint('Linear Regression Performance:')\nprint('R2 Score:', r2)\nprint('Mean Absolute Error:', mae)\nprint('Mean Squared Error:', mse)\n\n# Optionally, plot a scatter plot of predicted vs actual values\nplt.figure(figsize=(8, 6))\nplt.scatter(y_test, y_pred, alpha=0.7, color='orange')\nplt.xlabel('Actual Prices')\nplt.ylabel('Predicted Prices')\nplt.title('Actual vs Predicted Housing Prices')\nplt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], color='blue')\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T10:30:05.287593Z","iopub.execute_input":"2025-04-11T10:30:05.287881Z","iopub.status.idle":"2025-04-11T10:30:05.505560Z","shell.execute_reply.started":"2025-04-11T10:30:05.287861Z","shell.execute_reply":"2025-04-11T10:30:05.504703Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define the polynomial degree\ndegree = 2  # Quadratic model\n\n# Selecting the features for the polynomial regression\nfeatures_pol = ['codigo_comunidad','IPC','edad_media', 'mano_obra','num_hipotecas', 'num_inmigracion','precio_mats']\ntarget_pol = 'precio'\n\ndf_filtered_pol = df[features_pol + [target_pol]].dropna()\n\nX_pol = df_filtered_pol[features_pol]\ny_pol = df_filtered_pol[target_pol]\n\n\n# Create a polynomial regression model\npol_model = make_pipeline(PolynomialFeatures(degree), LinearRegression())\n\n# Fit the model\npol_model.fit(X_pol, y_pol)\n\n# Predict values\ny_pol_pred = pol_model.predict(X_pol)\n\n# Scatter plot of actual vs predicted income\nplt.figure(figsize=(8, 5))\nplt.scatter(y_pol, y_pol_pred, alpha=0.3, color=\"red\")\nplt.xlabel(\"Precio actual\")\nplt.ylabel(\"prevision del precio\")\nplt.title(f\"Polynomial Regression (Degree {degree}) - Actual vs Predicted Income\")\nplt.plot([y_pol.min(), y_pol.max()], [y_pol.min(), y_pol.max()], color='blue')\nplt.show()\n\n# Model score (R-squared value)\npol_r2_score = pol_model.score(X_pol, y_pol)\nprint(f\"Polynomial Regression R² Score (Degree {degree}): {pol_r2_score:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T10:32:42.912640Z","iopub.execute_input":"2025-04-11T10:32:42.912956Z","iopub.status.idle":"2025-04-11T10:32:43.168450Z","shell.execute_reply.started":"2025-04-11T10:32:42.912932Z","shell.execute_reply":"2025-04-11T10:32:43.167598Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# COMPARACION DE VARIOS ESTUDIOS A FUTURO DEL PRECIO","metadata":{}},{"cell_type":"code","source":"#comparacion de varios estudios\n#vovlemos a crear una regresion polinomica sin grafica esta vez\n\ndegree = 2\n\nfeatures = ['codigo_comunidad','IPC','edad_media', 'mano_obra','num_hipotecas', 'num_inmigracion','precio_mats']\ntarget = 'precio'\n\ndf_filter = df[features + [target]].dropna()\n\nX = df_filter[features]\ny = df_filter[target]\n\n\n\n\n#creamos un modelo de regresion polinomica\npol_model = make_pipeline(PolynomialFeatures(degree), LinearRegression())\n\n# Fit the model\npol_model.fit(X, y)\npol_r2 = pol_model.score(X, y)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T10:30:05.764486Z","iopub.execute_input":"2025-04-11T10:30:05.764779Z","iopub.status.idle":"2025-04-11T10:30:05.789056Z","shell.execute_reply.started":"2025-04-11T10:30:05.764758Z","shell.execute_reply":"2025-04-11T10:30:05.785751Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Random Forest Model\nrf_model = RandomForestRegressor(n_estimators=100, random_state=42)\nrf_model.fit(X, y)\nrf_r2 = rf_model.score(X, y)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T10:30:05.789707Z","iopub.execute_input":"2025-04-11T10:30:05.790003Z","iopub.status.idle":"2025-04-11T10:30:06.504763Z","shell.execute_reply.started":"2025-04-11T10:30:05.789981Z","shell.execute_reply":"2025-04-11T10:30:06.503869Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Gradient Boosting Model\ngb_model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, random_state=42)\ngb_model.fit(X, y)\ngb_r2 = gb_model.score(X, y)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T10:30:06.505652Z","iopub.execute_input":"2025-04-11T10:30:06.505963Z","iopub.status.idle":"2025-04-11T10:30:06.786099Z","shell.execute_reply.started":"2025-04-11T10:30:06.505937Z","shell.execute_reply":"2025-04-11T10:30:06.783093Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Feature Importance Analysis\nrf_feature_importance = rf_model.feature_importances_\ngb_feature_importance = gb_model.feature_importances_\n# Convert feature importances into DataFrame\nrf_importance_df = pd.DataFrame({\"Feature\": X.columns, \"Importance\": rf_feature_importance})\ngb_importance_df = pd.DataFrame({\"Feature\": X.columns, \"Importance\": gb_feature_importance})\n\n# Sort feature importances\nrf_importance_df = rf_importance_df.sort_values(by=\"Importance\", ascending=False)\ngb_importance_df = gb_importance_df.sort_values(by=\"Importance\", ascending=False)\n\n# Print R² scores\nprint(f\"Polynomial Regression R² Score: {pol_r2:.4f}\")\nprint(f\"Random Forest R² Score: {rf_r2:.4f}\")\nprint(f\"Gradient Boosting R² Score: {gb_r2:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T10:30:06.786800Z","iopub.execute_input":"2025-04-11T10:30:06.787059Z","iopub.status.idle":"2025-04-11T10:30:06.805291Z","shell.execute_reply.started":"2025-04-11T10:30:06.787037Z","shell.execute_reply":"2025-04-11T10:30:06.804349Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plot Feature Importance - Random Forest\nplt.figure(figsize=(10, 5))\nsns.barplot(x=rf_importance_df[\"Importance\"], y=rf_importance_df[\"Feature\"])\nplt.title(\"Feature Importance - Random Forest\")\nplt.xlabel(\"Importance Score\")\nplt.ylabel(\"Feature\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T10:30:06.806266Z","iopub.execute_input":"2025-04-11T10:30:06.807263Z","iopub.status.idle":"2025-04-11T10:30:06.994489Z","shell.execute_reply.started":"2025-04-11T10:30:06.807227Z","shell.execute_reply":"2025-04-11T10:30:06.993615Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plot Feature Importance - Gradient Boosting\nplt.figure(figsize=(10, 5))\nsns.barplot(x=gb_importance_df[\"Importance\"], y=gb_importance_df[\"Feature\"])\nplt.title(\"Feature Importance - Gradient Boosting\")\nplt.xlabel(\"Importance Score\")\nplt.ylabel(\"Feature\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T10:30:06.995338Z","iopub.execute_input":"2025-04-11T10:30:06.995631Z","iopub.status.idle":"2025-04-11T10:30:07.189975Z","shell.execute_reply.started":"2025-04-11T10:30:06.995611Z","shell.execute_reply":"2025-04-11T10:30:07.189026Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Split the dataset into 80% training and 20% testing\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Define hyperparameter grid for Random Forest\nrf_param_grid = {\n    \"n_estimators\": [50, 100, 200],  # Reduce the number of trees\n    \"max_depth\": [10, 20, None],  # Limit tree depth to prevent overfitting\n    \"min_samples_split\": [2, 5, 10],  # Minimum samples required to split a node\n    \"min_samples_leaf\": [1, 2, 4],  # Minimum samples required at a leaf node\n}\n\n# Define hyperparameter grid for Gradient Boosting\ngb_param_grid = {\n    \"n_estimators\": [50, 100, 200],  # Number of boosting stages\n    \"learning_rate\": [0.01, 0.1, 0.2],  # Lower learning rate for better generalization\n    \"max_depth\": [3, 5, 10],  # Depth of individual trees\n    \"min_samples_split\": [2, 5, 10],\n    \"min_samples_leaf\": [1, 2, 4],\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T10:30:07.191067Z","iopub.execute_input":"2025-04-11T10:30:07.191880Z","iopub.status.idle":"2025-04-11T10:30:07.200291Z","shell.execute_reply.started":"2025-04-11T10:30:07.191851Z","shell.execute_reply":"2025-04-11T10:30:07.198935Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Grid Search for Random Forest\nrf_grid_search = GridSearchCV(RandomForestRegressor(random_state=42), rf_param_grid, cv=3, n_jobs=-1, scoring=\"r2\")\nrf_grid_search.fit(X_train, y_train)\n\n# Grid Search for Gradient Boosting\ngb_grid_search = GridSearchCV(GradientBoostingRegressor(random_state=42), gb_param_grid, cv=3, n_jobs=-1, scoring=\"r2\")\ngb_grid_search.fit(X_train, y_train)\n\n# Best parameters found\nbest_rf_model = rf_grid_search.best_estimator_\nbest_gb_model = gb_grid_search.best_estimator_\n\nprint(f\"Best Random Forest Parameters: {rf_grid_search.best_params_}\")\nprint(f\"Best Gradient Boosting Parameters: {gb_grid_search.best_params_}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T10:30:07.201256Z","iopub.execute_input":"2025-04-11T10:30:07.201554Z","iopub.status.idle":"2025-04-11T10:32:14.515811Z","shell.execute_reply.started":"2025-04-11T10:30:07.201533Z","shell.execute_reply":"2025-04-11T10:32:14.514620Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Evaluate on the test set\nrf_r2_tuned = best_rf_model.score(X_test, y_test)\ngb_r2_tuned = best_gb_model.score(X_test, y_test)\n\nprint(f\"Tuned Random Forest R² Score (Test Set): {rf_r2_tuned:.4f}\")\nprint(f\"Tuned Gradient Boosting R² Score (Test Set): {gb_r2_tuned:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T10:32:14.517226Z","iopub.execute_input":"2025-04-11T10:32:14.517577Z","iopub.status.idle":"2025-04-11T10:32:14.548138Z","shell.execute_reply.started":"2025-04-11T10:32:14.517538Z","shell.execute_reply":"2025-04-11T10:32:14.547331Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define improved hyperparameter grid for Random Forest\nrf_param_grid = {\n    \"n_estimators\": [50, 100],  # Reduce number of trees\n    \"max_depth\": [5, 10],  # Force shallower trees\n    \"min_samples_split\": [10, 20],  # Require more samples per split\n    \"min_samples_leaf\": [4, 8],  # Require more samples per leaf\n    \"max_features\": [\"sqrt\", \"log2\"],  # Reduce number of features per tree\n}\n\n# Define improved hyperparameter grid for Gradient Boosting\ngb_param_grid = {\n    \"n_estimators\": [50, 100],  # Reduce number of boosting stages\n    \"learning_rate\": [0.01, 0.05],  # Lower learning rate for better generalization\n    \"max_depth\": [3, 5],  # Force shallower trees\n    \"min_samples_split\": [10, 20],  # Require more samples per split\n    \"min_samples_leaf\": [4, 8],  # Require more samples per leaf\n    \"subsample\": [0.7, 0.85],  # Randomly sample training data to reduce overfitting\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T10:32:14.549251Z","iopub.execute_input":"2025-04-11T10:32:14.549510Z","iopub.status.idle":"2025-04-11T10:32:14.555440Z","shell.execute_reply.started":"2025-04-11T10:32:14.549492Z","shell.execute_reply":"2025-04-11T10:32:14.554717Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Grid Search for Random Forest (more regularized)\nrf_grid_search = GridSearchCV(RandomForestRegressor(random_state=42), rf_param_grid, cv=3, n_jobs=-1, scoring=\"r2\")\nrf_grid_search.fit(X_train, y_train)\n\n# Grid Search for Gradient Boosting (more regularized)\ngb_grid_search = GridSearchCV(GradientBoostingRegressor(random_state=42), gb_param_grid, cv=3, n_jobs=-1, scoring=\"r2\")\ngb_grid_search.fit(X_train, y_train)\n\n# Best models\nbest_rf_model = rf_grid_search.best_estimator_\nbest_gb_model = gb_grid_search.best_estimator_\n\nprint(f\"Best Random Forest Parameters: {rf_grid_search.best_params_}\")\nprint(f\"Best Gradient Boosting Parameters: {gb_grid_search.best_params_}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T10:32:14.556475Z","iopub.execute_input":"2025-04-11T10:32:14.556828Z","iopub.status.idle":"2025-04-11T10:32:30.646346Z","shell.execute_reply.started":"2025-04-11T10:32:14.556799Z","shell.execute_reply":"2025-04-11T10:32:30.645679Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Evaluate on the test set\nrf_r2_tuned = best_rf_model.score(X_test, y_test)\ngb_r2_tuned = best_gb_model.score(X_test, y_test)\n\nprint(f\"Tuned (Regularized) Random Forest R² Score (Test Set): {rf_r2_tuned:.4f}\")\nprint(f\"Tuned (Regularized) Gradient Boosting R² Score (Test Set): {gb_r2_tuned:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T10:32:30.647209Z","iopub.execute_input":"2025-04-11T10:32:30.647976Z","iopub.status.idle":"2025-04-11T10:32:30.664882Z","shell.execute_reply.started":"2025-04-11T10:32:30.647955Z","shell.execute_reply":"2025-04-11T10:32:30.664200Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.linear_model import Ridge, Lasso\nfrom sklearn.model_selection import cross_val_score\n\n# Define hyperparameters for Ridge and Lasso Regression\nalpha_values = [0.01, 0.1, 1, 10, 100]  # Strength of regularization\n\n# Ridge Regression (L2 Regularization)\nridge_scores = []\nfor alpha in alpha_values:\n    ridge_model = Ridge(alpha=alpha)\n    ridge_model.fit(X_train, y_train)\n    score = ridge_model.score(X_test, y_test)\n    ridge_scores.append((alpha, score))\n\n# Lasso Regression (L1 Regularization)\nlasso_scores = []\nfor alpha in alpha_values:\n    lasso_model = Lasso(alpha=alpha, max_iter=10000)\n    lasso_model.fit(X_train, y_train)\n    score = lasso_model.score(X_test, y_test)\n    lasso_scores.append((alpha, score))\n\n# Print results\nprint(\"Ridge Regression R² Scores:\")\nfor alpha, score in ridge_scores:\n    print(f\"Alpha {alpha}: R² = {score:.4f}\")\n\nprint(\"\\nLasso Regression R² Scores:\")\nfor alpha, score in lasso_scores:\n    print(f\"Alpha {alpha}: R² = {score:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T10:32:30.666004Z","iopub.execute_input":"2025-04-11T10:32:30.666321Z","iopub.status.idle":"2025-04-11T10:32:30.743980Z","shell.execute_reply.started":"2025-04-11T10:32:30.666294Z","shell.execute_reply":"2025-04-11T10:32:30.743108Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Input, Dense\n# Define features and target variable\nfeatures = ['codigo_comunidad','IPC','edad_media', 'mano_obra','num_hipotecas', 'num_inmigracion','precio_mats']\ntarget = 'precio'\n\n\ndf_filtered = df[features + [target]].dropna()  # Remove missing values\nX = df_filtered[features]\ny = df_filtered[target]\n\n# Split the dataset into 80% training and 20% testing\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Scale the features (important for neural networks)\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# Define the Neural Network model\nmodel = Sequential([\n    Input(shape=(X_train_scaled.shape[1],)),\n    Dense(64, activation='relu'),  # Hidden layer 1\n    Dense(32, activation='relu'),  # Hidden layer 2\n    Dense(1)  # Output layer (regression task, so no activation function)\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T10:43:18.596304Z","iopub.execute_input":"2025-04-11T10:43:18.596782Z","iopub.status.idle":"2025-04-11T10:43:18.640140Z","shell.execute_reply.started":"2025-04-11T10:43:18.596748Z","shell.execute_reply":"2025-04-11T10:43:18.638954Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\n\n\n\n# Define a neural network\nmodeltensor = Sequential([\n    Input(shape=(X_train_scaled.shape[1],)),\n    Dense(64, activation='relu'),\n    Dense(1)  # salida para regresión\n])\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='mse', metrics=['mae'])\n\n# Train the model\nhistory = model.fit(X_train_scaled, y_train, epochs=100, batch_size=32, validation_data=(X_test_scaled, y_test), verbose=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T10:44:28.390173Z","iopub.execute_input":"2025-04-11T10:44:28.390868Z","iopub.status.idle":"2025-04-11T10:44:41.118074Z","shell.execute_reply.started":"2025-04-11T10:44:28.390846Z","shell.execute_reply":"2025-04-11T10:44:41.116875Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Evaluate the model\nloss, mae = model.evaluate(X_test_scaled, y_test)\nprint(f\"Neural Network Test MAE: {mae:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T10:44:54.493223Z","iopub.execute_input":"2025-04-11T10:44:54.493537Z","iopub.status.idle":"2025-04-11T10:44:54.588157Z","shell.execute_reply.started":"2025-04-11T10:44:54.493516Z","shell.execute_reply":"2025-04-11T10:44:54.587327Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Predict on test set\ny_pred = model.predict(X_test_scaled)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T10:44:56.618222Z","iopub.execute_input":"2025-04-11T10:44:56.618535Z","iopub.status.idle":"2025-04-11T10:44:56.782849Z","shell.execute_reply.started":"2025-04-11T10:44:56.618513Z","shell.execute_reply":"2025-04-11T10:44:56.781626Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Compute R² Score\nfrom sklearn.metrics import r2_score\nr2_nn = r2_score(y_test, y_pred)\nprint(f\"Neural Network R² Score: {r2_nn:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T10:44:58.662469Z","iopub.execute_input":"2025-04-11T10:44:58.662819Z","iopub.status.idle":"2025-04-11T10:44:58.669052Z","shell.execute_reply.started":"2025-04-11T10:44:58.662795Z","shell.execute_reply":"2025-04-11T10:44:58.667926Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save the trained model\nmodel.save(\"income_prediction_model.h5\")\nprint(\"Model saved successfully.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T10:45:01.156357Z","iopub.execute_input":"2025-04-11T10:45:01.156695Z","iopub.status.idle":"2025-04-11T10:45:01.180927Z","shell.execute_reply.started":"2025-04-11T10:45:01.156651Z","shell.execute_reply":"2025-04-11T10:45:01.180072Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#resultados de los diferentes modelos\n\nprint('\\nregresion polinomica:')\n\nprint(f\"Polynomial Regression R² Score: {pol_r2:.4f}\")\n\nprint('\\ngradiente y bosque aleatorio:')\n\nprint(f\"Tuned (Regularized) Random Forest R² Score (Test Set): {rf_r2_tuned:.4f}\")\nprint(f\"Tuned (Regularized) Gradient Boosting R² Score (Test Set): {gb_r2_tuned:.4f}\")\n\nprint('\\nridge y lasso:')\n\nprint(\"Ridge Regression R² Scores:\")\nfor alpha, score in ridge_scores:\n    print(f\"Alpha {alpha}: R² = {score:.4f}\")\n\nprint(\"\\nLasso Regression R² Scores:\")\nfor alpha, score in lasso_scores:\n    print(f\"Alpha {alpha}: R² = {score:.4f}\")\n\nprint('\\nred neuronal:')\n\nprint(f\"Neural Network R² Score: {r2_nn:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T10:45:05.227870Z","iopub.execute_input":"2025-04-11T10:45:05.228171Z","iopub.status.idle":"2025-04-11T10:45:05.235535Z","shell.execute_reply.started":"2025-04-11T10:45:05.228151Z","shell.execute_reply":"2025-04-11T10:45:05.234651Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Gradient Boosting y Random Forest: Estos modelos son altamente efectivos, con R² superiores a 0.90, y deberían ser considerados como las opciones principales para la predicción de precios en este caso.\n\nRegresión Polinómica: Aunque este modelo no supera a los modelos de ensemble, tiene un buen rendimiento (R² = 0.7478) y puede ser útil si se busca una solución más sencilla y fácil de interpretar, aunque con un rendimiento algo inferior.\n\nModelos Lineales (Ridge y Lasso): No aportan mejoras significativas en comparación con otros modelos. Ridge y Lasso con los diferentes valores de alpha no logran un buen ajuste (siendo el R² consistentemente bajo), lo que podría indicar que una regularización más fuerte no es beneficiosa en este caso.\n\nRed Neuronal: Aunque tiene un rendimiento moderado (R² = 0.6091), está por debajo de los modelos de ensemble. Esto sugiere que este conjunto de datos no es el más adecuado para una red neuronal, o que la red necesita ajustes adicionales (como más capas, diferentes funciones de activación o mayor cantidad de datos).","metadata":{}}]}